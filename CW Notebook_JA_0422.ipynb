{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ECS6P9U/P: NEURAL NETWORKS & DEEP LEARNING\n",
    "\n",
    "2021/22 – Semester 2\n",
    "\n",
    "Dr. Yorgos Tzimiropoulos\n",
    "\n",
    "Spring 2022 - Coursework (Notebook)\n",
    "\n",
    "Jordan Akisanya – 200884501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import my_utils as mu\n",
    "import torch\n",
    "from torch import nn\n",
    "from IPython import display\n",
    "import pandas as pd\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from collections import OrderedDict, namedtuple\n",
    "from itertools import product\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Create the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "    def __init__(self, num_patches, num_channels, patches_mlp_dim, channels_mlp_dim, dropout_p):\n",
    "        super(MixerBlock, self).__init__()\n",
    "        self.num_patches = num_patches\n",
    "        self.num_channels = num_channels\n",
    "        self.patches_mlp_dim = patches_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "        self.dropout_p = dropout_p\n",
    "\n",
    "        # (1) token-mixing MLP\n",
    "        self.MLP1 = nn.Sequential(nn.Linear(self.num_patches, self.patches_mlp_dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(self.dropout_p), \n",
    "                                  nn.Linear(self.patches_mlp_dim, self.num_patches),\n",
    "                                  nn.Dropout(self.dropout_p))\n",
    "        # (2) channel-mixing MLP\n",
    "        self.MLP2 = nn.Sequential(nn.Linear(self.num_channels, self.channels_mlp_dim),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Dropout(self.dropout_p), \n",
    "                                  nn.Linear(self.channels_mlp_dim, self.num_channels),\n",
    "                                  nn.Dropout(self.dropout_p))\n",
    "\n",
    "        self.norm1 = nn.LayerNorm(self.num_channels)\n",
    "        self.norm2 = nn.LayerNorm(self.num_channels)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y = self.norm1(x)\n",
    "        y = y.transpose(2, 1)\n",
    "        y = self.MLP1(y) \n",
    "        y = y.transpose(2, 1) \n",
    "        x = x + y             \n",
    "        y = self.norm2(x) \n",
    "        return x + self.MLP2(y) \n",
    "    \n",
    "class MlpMixer(nn.Module):\n",
    "    def __init__(self, num_classes, num_blocks, patch_size, num_patches, \n",
    "                 num_channels, patches_mlp_dim, channels_mlp_dim, dropout_p):\n",
    "        super(MlpMixer, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.num_blocks = num_blocks\n",
    "        self.patch_size = patch_size\n",
    "        self.num_patches = num_patches\n",
    "        self.num_channels = num_channels\n",
    "        self.patches_mlp_dim = patches_mlp_dim\n",
    "        self.channels_mlp_dim = channels_mlp_dim\n",
    "        self.dropout_p = dropout_p\n",
    "        \n",
    "        self.MixerBlock = MixerBlock(self.num_patches, self.num_channels,\n",
    "                                     self.patches_mlp_dim, self.channels_mlp_dim, \n",
    "                                     self.dropout_p)\n",
    "        \n",
    "        self.stem_mlp = nn.Sequential(nn.Linear(self.num_channels, self.num_channels),\n",
    "                                      nn.ReLU())\n",
    "            \n",
    "        self.norm1 = nn.LayerNorm(num_channels)\n",
    "        self.out = nn.Linear(self.num_channels, self.num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Stem\n",
    "        x = mu.F.unfold(x, kernel_size=self.patch_size, stride=self.patch_size)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.stem_mlp(x)\n",
    "\n",
    "        # Backbone\n",
    "        for _ in range(self.num_blocks): \n",
    "            x = self.MixerBlock(x)\n",
    "        x = self.norm1(x)\n",
    "\n",
    "        # Logits out\n",
    "        x = torch.mean(x, dim=1)\n",
    "        logits = self.out(x)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training and Evaluation Functions/Classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accuracy function\n",
    "# Source: my_utils\n",
    "def accuracy(y_hat, y):  #y_hat is a matrix; 2nd dimension stores prediction scores for each class.\n",
    "    \"\"\"Compute the number of correct predictions.\"\"\"\n",
    "    if len(y_hat.shape) > 1 and y_hat.shape[1] > 1:\n",
    "        y_hat = y_hat.argmax(axis=1) # Predicted class is the index of max score         \n",
    "    cmp = (y_hat.type(y.dtype) == y)  # because`==` is sensitive to data types\n",
    "    return float(torch.sum(cmp)) # Taking the sum yields the number of correct predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accumulator class\n",
    "# Source: my_utils\n",
    "class Accumulator:  \n",
    "    \"\"\"For accumulating sums over `n` variables.\"\"\"\n",
    "    def __init__(self, n):\n",
    "        self.data = [0.0] * n # [0, 0, ..., 0]\n",
    "    def add(self, *args):\n",
    "        self.data = [a + float(b) for a, b in zip(self.data, args)]\n",
    "    def reset(self):\n",
    "        self.data = [0.0] * len(self.data)\n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define accuracy evaluation function\n",
    "# Source: my_utils \n",
    "def evaluate_accuracy(net, data_iter, device): \n",
    "    \"\"\"Compute the accuracy for a model on a dataset.\"\"\"\n",
    "    metric = Accumulator(2)  # No. of correct predictions, no. of predictions\n",
    "    for _, (X, y) in enumerate(data_iter):\n",
    "        (X, y) = (X.to(device), y.to(device))\n",
    "        metric.add(accuracy(net(X), y), y.numel())\n",
    "    return metric[0] / metric[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define training function \n",
    "# Source: my_utils\n",
    "def train_epoch_ch3(net, train_iter, loss, optimizer, device):  \n",
    "    \"\"\"The training function for one epoch.\"\"\"\n",
    "    # Set the model to training mode\n",
    "    if isinstance(net, torch.nn.Module):\n",
    "        net.train()\n",
    "    # Sum of training loss, sum of training accuracy, no. of examples\n",
    "    metric = Accumulator(3)\n",
    "    for X, y in train_iter:\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        # Compute gradients and update parameters\n",
    "        y_hat = net(X)\n",
    "        l = loss(y_hat, y)\n",
    "        optimizer.zero_grad()\n",
    "        l.backward()\n",
    "        optimizer.step()\n",
    "        metric.add(float(l) * len(y), accuracy(y_hat, y), y.size().numel())\n",
    "    # Return training loss and training accuracy\n",
    "    return metric[0] / metric[2], metric[1] / metric[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the epoch class\n",
    "# Source: github.com/ElisonSherton/Deep_Learning_Using_PyTorch\n",
    "class Epoch():\n",
    "    def __init__(self):\n",
    "        # Keeps track of which epoch it is\n",
    "        self.count = 0\n",
    "        # Keeps track of the loss\n",
    "        self.loss = 0\n",
    "        # Keeps track of the number of correct predictions\n",
    "        self.num_correct = 0\n",
    "        # When to start\n",
    "        self.start_time = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the run manager class\n",
    "# Source https://github.com/ElisonSherton/Deep_Learning_Using_PyTorch\n",
    "# Modified to fit existing my_utils function outputs\n",
    "class RunManager():\n",
    "    def __init__(self):\n",
    "        self.e = Epoch()\n",
    "        self.run_params = None\n",
    "        self.run_count = 0\n",
    "        self.run_data = []\n",
    "        self.run_start_time = None\n",
    "        self.network = None\n",
    "        self.loader = None\n",
    "        self.tb = None\n",
    "    \n",
    "    def begin_run(self, run, network, loader):\n",
    "        self.run_start_time = time.time()\n",
    "        self.run_params = run\n",
    "        self.run_count += 1\n",
    "        self.network = network\n",
    "        self.loader = loader\n",
    "        self.tb = SummaryWriter(comment = f\"-{run}\")\n",
    "#           self.tb.add_graph(network)\n",
    "    \n",
    "    def end_run(self):\n",
    "        self.tb.close()\n",
    "        self.e.count = 0\n",
    "    \n",
    "    def begin_epoch(self):\n",
    "        self.e.start_time = time.time()\n",
    "        self.e.count += 1\n",
    "#           self.e.loss = 0\n",
    "#           self.e.num_correct = 0\n",
    "        \n",
    "    def end_epoch(self):\n",
    "        epoch_duration = time.time() - self.e.start_time\n",
    "        run_duration = time.time() - self.run_start_time\n",
    "#           loss = self.e.loss / len(self.loader.dataset)\n",
    "#           accuracy = self.e.num_correct / (len(self.loader.dataset))\n",
    "        loss = train_loss\n",
    "        train_accuracy = train_acc\n",
    "        test_accuracy = test_acc\n",
    "        \n",
    "        self.tb.add_scalar('Loss', loss, self.e.count)\n",
    "        self.tb.add_scalar('Train_Accuracy', train_accuracy, self.e.count)\n",
    "        self.tb.add_scalar('Test_Accuracy', test_accuracy, self.e.count)\n",
    "\n",
    "        for name, param in self.network.named_parameters():\n",
    "            self.tb.add_histogram(name, param, self.e.count)\n",
    "            self.tb.add_histogram(f\"{name}.grad\", param.grad, self.e.count)\n",
    "            \n",
    "        results = {'Run': self.run_count, \n",
    "                   'Epoch': self.e.count, \n",
    "                   'Loss': loss,\n",
    "                   'Train_Accuracy': train_accuracy,\n",
    "                   'Test_Accuracy': test_accuracy,\n",
    "                   'Epoch Duration': epoch_duration,\n",
    "                   'Run Duration': run_duration}\n",
    "        \n",
    "        for k, v in self.run_params._asdict().items(): \n",
    "            results[k] = v\n",
    "        \n",
    "        self.run_data.append(results)\n",
    "\n",
    "        df = pd.DataFrame.from_dict(self.run_data, orient='columns')\n",
    "        display.clear_output(wait = True)\n",
    "        print(df)\n",
    "        \n",
    "    def track_loss(self, loss):\n",
    "        self.e.loss += loss.item() * self.loader.batch_size\n",
    "    \n",
    "    def track_num_correct(self, pred, labels):\n",
    "        self.e.num_correct += pred.argmax(dim = 1).eq(labels).sum().item()\n",
    "        \n",
    "    def save(self, fileName):\n",
    "        pd.DataFrame.from_dict(\n",
    "            self.run_data, orient='columns'\n",
    "        ).to_csv(f'{fileName}.csv')\n",
    "\n",
    "        with open(f'{fileName}.json', 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.run_data, f, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the parameter builder class\n",
    "# Source: github.com/ElisonSherton/Deep_Learning_Using_PyTorch \n",
    "class RunBuilder():\n",
    "    @staticmethod\n",
    "    def get_runs(params):\n",
    "        \n",
    "        # Creates a named tuple which we can use to access values in organized way of .notation\n",
    "        Run = namedtuple('Run', params.keys())\n",
    "        # Create a container to hold all the combination of params\n",
    "        runs = []\n",
    "        \n",
    "        # Compute the Cartesian product of parameters\n",
    "        for v in product(*params.values()):\n",
    "            runs.append(Run(*v))\n",
    "        \n",
    "        return runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the parameter dictionary \n",
    "params = OrderedDict(\n",
    "    lr = [.01],\n",
    "    dropout_p = [.1],\n",
    "    batch_size = [256],\n",
    "    num_epochs = [60],\n",
    "    device = ['cpu']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Run  Epoch      Loss  Train_Accuracy  Test_Accuracy  Epoch Duration  \\\n",
      "0     1      1  0.709132        0.742000         0.8097       17.769782   \n",
      "1     1      2  0.447118        0.836700         0.8239       15.865147   \n",
      "2     1      3  0.405036        0.849417         0.8464       15.940517   \n",
      "3     1      4  0.377228        0.862250         0.8548       15.856584   \n",
      "4     1      5  0.354748        0.869200         0.8525       15.919687   \n",
      "5     1      6  0.345007        0.873533         0.8599       15.758570   \n",
      "6     1      7  0.332167        0.877633         0.8626       15.906396   \n",
      "7     1      8  0.324218        0.880933         0.8646       16.005501   \n",
      "8     1      9  0.313348        0.884283         0.8618       15.955086   \n",
      "9     1     10  0.307857        0.886900         0.8732       15.820107   \n",
      "10    1     11  0.299736        0.888700         0.8711       15.811812   \n",
      "11    1     12  0.296812        0.888567         0.8720       15.887808   \n",
      "12    1     13  0.287921        0.892400         0.8774       15.788039   \n",
      "13    1     14  0.284071        0.895000         0.8727       15.787641   \n",
      "14    1     15  0.279360        0.896683         0.8788       15.726525   \n",
      "15    1     16  0.274563        0.897850         0.8781       15.729531   \n",
      "16    1     17  0.269700        0.899767         0.8786       15.828112   \n",
      "17    1     18  0.266254        0.901917         0.8769       15.821107   \n",
      "18    1     19  0.261061        0.902633         0.8806       15.785916   \n",
      "19    1     20  0.258680        0.903383         0.8792       15.802445   \n",
      "20    1     21  0.254197        0.904100         0.8778       15.890387   \n",
      "21    1     22  0.253498        0.904067         0.8768       15.777122   \n",
      "22    1     23  0.244625        0.909383         0.8803       16.002263   \n",
      "23    1     24  0.243794        0.909950         0.8739       15.980747   \n",
      "24    1     25  0.239484        0.909583         0.8835       15.813396   \n",
      "25    1     26  0.239136        0.910983         0.8813       15.989884   \n",
      "26    1     27  0.233000        0.911383         0.8810       15.820666   \n",
      "27    1     28  0.230724        0.912767         0.8860       15.847630   \n",
      "28    1     29  0.227058        0.914650         0.8774       15.935205   \n",
      "29    1     30  0.225458        0.915967         0.8841       15.963903   \n",
      "30    1     31  0.219226        0.917283         0.8800       15.998999   \n",
      "31    1     32  0.220014        0.917350         0.8840       15.942211   \n",
      "32    1     33  0.218259        0.917217         0.8857       15.859479   \n",
      "33    1     34  0.215062        0.918550         0.8821       15.934468   \n",
      "34    1     35  0.211191        0.920033         0.8844       15.998761   \n",
      "35    1     36  0.208579        0.922800         0.8808       15.881159   \n",
      "36    1     37  0.207851        0.922283         0.8807       15.892672   \n",
      "37    1     38  0.204924        0.923183         0.8809       16.010874   \n",
      "38    1     39  0.204987        0.922983         0.8803       15.889923   \n",
      "39    1     40  0.198682        0.925767         0.8798       15.754478   \n",
      "40    1     41  0.199133        0.925767         0.8826       16.172911   \n",
      "41    1     42  0.194060        0.926633         0.8816       15.840552   \n",
      "42    1     43  0.193286        0.927267         0.8826       16.001823   \n",
      "43    1     44  0.192766        0.928317         0.8823       15.884662   \n",
      "44    1     45  0.192977        0.927650         0.8814       15.956321   \n",
      "45    1     46  0.185883        0.930267         0.8834       15.791582   \n",
      "46    1     47  0.184923        0.931383         0.8854       15.846629   \n",
      "47    1     48  0.184309        0.931067         0.8882       15.939361   \n",
      "48    1     49  0.183661        0.930483         0.8842       15.810978   \n",
      "49    1     50  0.182217        0.931783         0.8838       15.823774   \n",
      "50    1     51  0.176288        0.934133         0.8793       15.784808   \n",
      "51    1     52  0.174945        0.934200         0.8812       15.729028   \n",
      "52    1     53  0.175480        0.935033         0.8865       15.734182   \n",
      "53    1     54  0.171020        0.936733         0.8789       15.835622   \n",
      "54    1     55  0.171472        0.937317         0.8867       15.729063   \n",
      "55    1     56  0.167343        0.937283         0.8836       15.780811   \n",
      "56    1     57  0.166485        0.937233         0.8833       15.770385   \n",
      "57    1     58  0.166606        0.937200         0.8883       15.772248   \n",
      "58    1     59  0.164512        0.938450         0.8842       15.879658   \n",
      "59    1     60  0.158312        0.940267         0.8777       15.904336   \n",
      "\n",
      "    Run Duration    lr  dropout_p  batch_size  num_epochs device  \n",
      "0      17.775788  0.01        0.1         256          60   cuda  \n",
      "1      33.784358  0.01        0.1         256          60   cuda  \n",
      "2      49.859489  0.01        0.1         256          60   cuda  \n",
      "3      65.856693  0.01        0.1         256          60   cuda  \n",
      "4      81.911109  0.01        0.1         256          60   cuda  \n",
      "5      97.803294  0.01        0.1         256          60   cuda  \n",
      "6     113.843806  0.01        0.1         256          60   cuda  \n",
      "7     129.987924  0.01        0.1         256          60   cuda  \n",
      "8     146.089636  0.01        0.1         256          60   cuda  \n",
      "9     162.041355  0.01        0.1         256          60   cuda  \n",
      "10    177.987782  0.01        0.1         256          60   cuda  \n",
      "11    194.009704  0.01        0.1         256          60   cuda  \n",
      "12    209.930857  0.01        0.1         256          60   cuda  \n",
      "13    225.853615  0.01        0.1         256          60   cuda  \n",
      "14    241.710250  0.01        0.1         256          60   cuda  \n",
      "15    257.571394  0.01        0.1         256          60   cuda  \n",
      "16    273.534307  0.01        0.1         256          60   cuda  \n",
      "17    289.486217  0.01        0.1         256          60   cuda  \n",
      "18    305.401744  0.01        0.1         256          60   cuda  \n",
      "19    321.335301  0.01        0.1         256          60   cuda  \n",
      "20    337.360344  0.01        0.1         256          60   cuda  \n",
      "21    353.270152  0.01        0.1         256          60   cuda  \n",
      "22    369.402526  0.01        0.1         256          60   cuda  \n",
      "23    385.516388  0.01        0.1         256          60   cuda  \n",
      "24    401.469402  0.01        0.1         256          60   cuda  \n",
      "25    417.591989  0.01        0.1         256          60   cuda  \n",
      "26    433.549772  0.01        0.1         256          60   cuda  \n",
      "27    449.530975  0.01        0.1         256          60   cuda  \n",
      "28    465.601797  0.01        0.1         256          60   cuda  \n",
      "29    481.700316  0.01        0.1         256          60   cuda  \n",
      "30    497.843439  0.01        0.1         256          60   cuda  \n",
      "31    513.928271  0.01        0.1         256          60   cuda  \n",
      "32    529.920363  0.01        0.1         256          60   cuda  \n",
      "33    545.986444  0.01        0.1         256          60   cuda  \n",
      "34    562.119319  0.01        0.1         256          60   cuda  \n",
      "35    578.135092  0.01        0.1         256          60   cuda  \n",
      "36    594.160378  0.01        0.1         256          60   cuda  \n",
      "37    610.307367  0.01        0.1         256          60   cuda  \n",
      "38    626.329903  0.01        0.1         256          60   cuda  \n",
      "39    642.219999  0.01        0.1         256          60   cuda  \n",
      "40    658.528025  0.01        0.1         256          60   cuda  \n",
      "41    674.507194  0.01        0.1         256          60   cuda  \n",
      "42    690.645132  0.01        0.1         256          60   cuda  \n",
      "43    706.662759  0.01        0.1         256          60   cuda  \n",
      "44    722.754196  0.01        0.1         256          60   cuda  \n",
      "45    738.680894  0.01        0.1         256          60   cuda  \n",
      "46    754.665142  0.01        0.1         256          60   cuda  \n",
      "47    770.739119  0.01        0.1         256          60   cuda  \n",
      "48    786.685478  0.01        0.1         256          60   cuda  \n",
      "49    802.642867  0.01        0.1         256          60   cuda  \n",
      "50    818.563209  0.01        0.1         256          60   cuda  \n",
      "51    834.428854  0.01        0.1         256          60   cuda  \n",
      "52    850.296650  0.01        0.1         256          60   cuda  \n",
      "53    866.269390  0.01        0.1         256          60   cuda  \n",
      "54    882.133852  0.01        0.1         256          60   cuda  \n",
      "55    898.048779  0.01        0.1         256          60   cuda  \n",
      "56    913.955282  0.01        0.1         256          60   cuda  \n",
      "57    929.865648  0.01        0.1         256          60   cuda  \n",
      "58    945.880772  0.01        0.1         256          60   cuda  \n",
      "59    961.920224  0.01        0.1         256          60   cuda  \n"
     ]
    }
   ],
   "source": [
    "m = RunManager()\n",
    "for run in RunBuilder.get_runs(params):\n",
    "    # Task 1 - Read dataset and create dataloader\n",
    "    loader = mu.load_data_fashion_mnist(batch_size=run.batch_size)\n",
    "\n",
    "    # Task 2 - Create the model\n",
    "    net = MlpMixer(10, 8, (4,4), 49, 16, 1024, 512, run.dropout_p).to(run.device)\n",
    "    \n",
    "    # Task 3 - Create the loss and optimizer\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=run.lr)\n",
    "\n",
    "    # Task 4 - Write the training script and train the model\n",
    "    m.begin_run(run, net, loader)\n",
    "    for epoch in range(run.num_epochs):\n",
    "        m.begin_epoch()\n",
    "        train_iter, test_iter = loader\n",
    "        train_metrics = train_epoch_ch3(net, train_iter, loss, optimizer, run.device)\n",
    "        test_acc = evaluate_accuracy(net, test_iter, run.device)\n",
    "        train_loss, train_acc = train_metrics\n",
    "        m.end_epoch()\n",
    "    m.end_run()\n",
    "m.save('results_1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Display the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Run</th>\n",
       "      <th>Epoch</th>\n",
       "      <th>Loss</th>\n",
       "      <th>Train_Accuracy</th>\n",
       "      <th>Test_Accuracy</th>\n",
       "      <th>Epoch Duration</th>\n",
       "      <th>Run Duration</th>\n",
       "      <th>lr</th>\n",
       "      <th>dropout_p</th>\n",
       "      <th>batch_size</th>\n",
       "      <th>num_epochs</th>\n",
       "      <th>device</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>58</td>\n",
       "      <td>0.166606</td>\n",
       "      <td>0.937200</td>\n",
       "      <td>0.8883</td>\n",
       "      <td>15.772248</td>\n",
       "      <td>929.865648</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>48</td>\n",
       "      <td>0.184309</td>\n",
       "      <td>0.931067</td>\n",
       "      <td>0.8882</td>\n",
       "      <td>15.939361</td>\n",
       "      <td>770.739119</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>55</td>\n",
       "      <td>0.171472</td>\n",
       "      <td>0.937317</td>\n",
       "      <td>0.8867</td>\n",
       "      <td>15.729063</td>\n",
       "      <td>882.133852</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>53</td>\n",
       "      <td>0.175480</td>\n",
       "      <td>0.935033</td>\n",
       "      <td>0.8865</td>\n",
       "      <td>15.734182</td>\n",
       "      <td>850.296650</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>28</td>\n",
       "      <td>0.230724</td>\n",
       "      <td>0.912767</td>\n",
       "      <td>0.8860</td>\n",
       "      <td>15.847630</td>\n",
       "      <td>449.530975</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.1</td>\n",
       "      <td>256</td>\n",
       "      <td>60</td>\n",
       "      <td>cuda</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Run  Epoch      Loss  Train_Accuracy  Test_Accuracy  Epoch Duration  \\\n",
       "57    1     58  0.166606        0.937200         0.8883       15.772248   \n",
       "47    1     48  0.184309        0.931067         0.8882       15.939361   \n",
       "54    1     55  0.171472        0.937317         0.8867       15.729063   \n",
       "52    1     53  0.175480        0.935033         0.8865       15.734182   \n",
       "27    1     28  0.230724        0.912767         0.8860       15.847630   \n",
       "\n",
       "    Run Duration    lr  dropout_p  batch_size  num_epochs device  \n",
       "57    929.865648  0.01        0.1         256          60   cuda  \n",
       "47    770.739119  0.01        0.1         256          60   cuda  \n",
       "54    882.133852  0.01        0.1         256          60   cuda  \n",
       "52    850.296650  0.01        0.1         256          60   cuda  \n",
       "27    449.530975  0.01        0.1         256          60   cuda  "
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Display the results\n",
    "pd.DataFrame.from_dict(m.run_data, orient='columns').sort_values(\"Test_Accuracy\", ascending=False).head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard\n",
    "%tensorboard --logdir runs/"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "67879629391049e40c4a9000847b9980170954d43b8414027d94ec45ae266a71"
  },
  "kernelspec": {
   "display_name": "Python 3.9.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
